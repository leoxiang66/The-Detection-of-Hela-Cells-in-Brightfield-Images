{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch introduction\n",
    "The fundamental building block in PyTorch is *tensors*. A PyTorch Tensor is conceptually identical to a numpy array. Any computation you might want to perform with numpy can also be accomplished with PyTorch Tensors.\n",
    "\n",
    "Unlike NumPy, Tensors can utilize GPUs to accelerate numeric computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random matrix\n",
    "x = torch.randn(2,3, device=device)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(x,x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x @ x.T # Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x * x # Elementwise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Autograd\n",
    "Manually implementing the backward pass for large networks can quickly get complex. \n",
    "With *automatic differentiation*, this can be done automatically using `autograd` in PyTorch. The forward pass in a network will define a computational graph, where nodes will be Tensors and edges will be functions that produce output Tensors from the input Tensors. \n",
    "\n",
    "The only thing we need to do it specifying `requires_grad=True` when constructing a Tensor.\n",
    "\n",
    "With `x` being a tensor with `requires_grad=True`, after backpropagation `x.grad` will be a Tensor holding the gradient of `x` with respect to some scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([0.1], requires_grad=True)\n",
    "b = torch.tensor([2.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.0])\n",
    "y = torch.tensor([1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass: compute predicted y using operations on Tensors. \n",
    "\n",
    "Since w and b have requires_grad=True, operations involving these Tensors will cause\n",
    "PyTorch to build a computational graph, allowing automatic computation of\n",
    "gradients. \n",
    "\n",
    "Since we are no longer implementing the backward pass by hand we\n",
    "don't need to keep references to intermediate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w*x+b # Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'True label: {y}', f'\\nPredicted: {y_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (y_pred - y).pow(2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(f'Gradient b: {b.grad}')\n",
    "print(f'Gradient w: {w.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually zero the gradients after running the backward pass\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: nn\n",
    "For large neural networks, raw autograd can be a bit too low-level. \n",
    "\n",
    "When building neural networks we usually arrange the computation into layers, wheras some have learnable parameters to be optimized during learning. \n",
    "\n",
    "In PyTorch the `nn` package defines a set of *Modules* which are roughly equivalent to neural network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = 64 # N is batch size\n",
    "D_in = 1000 # D_in is input dimension\n",
    "H = 100 # H is hidden dimension\n",
    "D_out = 10 # D_out is output dimension.\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss() # reduction = none, default=mean, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x)\n",
    "y_pred[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(y_pred, y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "# we can access its data and gradients like we did before.\n",
    "# Example of parameter update\n",
    "learning_rate = 1e-1\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Custom nn Modules\n",
    "You can define your own Modules by subclassing `nn.Module` and defining the `forward` pass.\n",
    "\n",
    "### Constructor:\n",
    "In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "### Forward function:\n",
    "In the forward function we accept a Tensor of input data and we must return a Tensor of output data. \n",
    "We can use Modules defined in the constructor as well as arbitrary (differentiable) operations on Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    super(TwoLayerNet, self).__init__()\n",
    "    self.linear1 = torch.nn.Linear(D_in, H)\n",
    "    self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "  def forward(self, x):\n",
    "    relu = torch.nn.ReLU()\n",
    "    h_relu = relu(self.linear1(x))\n",
    "    y_pred = self.linear2(h_relu)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimization algorithm to be used (Stochastic Gradient Descent):\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(y_pred, y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero gradients, perform a backward pass, and update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
