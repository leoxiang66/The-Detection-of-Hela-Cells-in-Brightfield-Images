{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "Using PyTorch to perform linear regression.\n",
    "An official introduction to the PyTorch library can be found here: https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression - Manual implementation in PyTorch\n",
    "In the below example we see a 1D linear regression example: $y=wx+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(range(-5,5)).float()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gt = 3\n",
    "b_gt = 4\n",
    "y = w_gt*x + b_gt\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "print(f'w: {w}')\n",
    "print(f'b: {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out with different learning rate.\n",
    "What happens if learning rate $0.1$ is used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainManual(w, b, lr, iternum):\n",
    "    for i in range(iternum):\n",
    "        y_hat = w*x + b\n",
    "\n",
    "        error = torch.sum(torch.pow(y-y_hat,2)/y.numel())\n",
    "        error.backward()   # Compute the Gradients for w and b (requires_grad=True)\n",
    "        print(w,w.grad)\n",
    "        # Update parameters\n",
    "        with torch.no_grad():   # Temporarily set all requires_grad=False\n",
    "            w -= lr * w.grad\n",
    "            b -= lr * b.grad\n",
    "            # Remember to zero the gradients!\n",
    "            # If not, the gradients will be accumulated\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "        print(\"Error: {:.4f}\".format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainManual(w, b, learning_rate, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (w*x + b)\n",
    "y_pred = y_pred.data.numpy()\n",
    "print(\"----- ----- ----- ----- -----\")\n",
    "print(\"Prediction:\")\n",
    "print(\"w_pred = {:.2f}, b_pred = {:.2f}\".format(w[0] ,b[0]))\n",
    "print(\"Ground-truth:\")\n",
    "print(\"w_gt = {:.2f}, b_gt = {:.2f}\".format(w_gt ,b_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(x, y, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x, y_pred, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression - Using the torch.nn.Module\n",
    "In the below example we see a 1D linear regression example: $y=wx+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module): \n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__() \n",
    "        self.linear = torch.nn.Linear(1, 1, bias = True) # bias is default True\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(our_model.parameters(), lr = 0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBuildIn(model, x, y, iter):\n",
    "    for i in range(iter):\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get output from the model, given the inputs\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(y_pred, y)\n",
    "        print(loss)\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Iter {}, loss {}'.format(iter, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[:,None]\n",
    "y_train = y[:,None]\n",
    "\n",
    "print(x.shape, x_train.shape)\n",
    "print(y.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBuildIn(our_model, x_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bi = our_model(x_train).data.numpy()\n",
    "\n",
    "print(\"----- ----- ----- ----- -----\")\n",
    "print(\"Prediction:\")\n",
    "for name, param in our_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "print(\"Ground-truth:\")\n",
    "print(\"w_gt = {:.2f}, b_gt = {:.2f}\".format(w_gt ,b_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(x, y, 'go', label='True data', alpha=0.5)\n",
    "plt.plot(x, y_pred_bi, '--', label='Predictions', alpha=0.5)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation with SKLEARN\n",
    "Using \"Ordinary least squares Linear Regression\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x[:,None], y[:,None])\n",
    "print(\"----- ----- ----- ----- -----\")\n",
    "print(\"Prediction:\")\n",
    "print(\"w_pred = {:.2f}, b_pred = {:.2f}\".format(float(linr.intercept_), float(linr.coef_[0])))\n",
    "print(\"Ground-truth:\")\n",
    "print(\"w_gt = {:.2f}, b_gt = {:.2f}\".format(w_gt ,b_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
